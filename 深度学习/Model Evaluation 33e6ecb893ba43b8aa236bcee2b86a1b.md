# Model Evaluation

# 初级

Q1：***解释机器学习中的过拟合现象？***

- **过拟合**是指模型对训练数据建模得太好。
- 当模型学习训练数据中的细节和噪声达到对模型在新数据上的性能产生负面影响的程度时，就会发生**过度拟合。**这意味着训练数据中的噪声或随机波动被模型拾取并学习为概念，问题是这些概念并不适用于新数据并对模型的泛化能力产生负面影响

![Untitled](Model%20Evaluation%2033e6ecb893ba43b8aa236bcee2b86a1b/Untitled.png)

Q2：***解释机器学习中的欠拟合现象***

- **欠拟合**是指既不能对训练数据良好建模也不能泛化到新数据的模型。欠拟合的机器学习模型不是合适的模型，因为它在训练数据上的表现不佳。
- **欠拟合**通常不被讨论，因为在给定良好性能指标的情况下很容易检测到。欠拟合的补救措施是使用更适合的机器学习算法

Q3：***什么是hyper-parameters？***

- 每个机器学习模型都有参数，还*可以*有超参数，参数通过模型训练更新。**超参数**是那些不能从常规训练过程中直接学习的参数**。**这些参数表示模型**的高级**属性，例如它的*复杂性***或**它*应该学习的速度*。

Q4：***如何检测到机器学习中的过拟合现象？***

- **过拟合**的常见模式可以在**学习曲线**图上看到，其中模型在训练数据集上的性能不断提高，但在测试或验证集上的性能提高到一个点然后开始变差。因此，过拟合模型的**训练误差极低，但测试误差却很高**。

![Untitled](Model%20Evaluation%2033e6ecb893ba43b8aa236bcee2b86a1b/Untitled%201.png)

Q5：***什么是模型学习率？学习率高或低对模型学习有什么影响？***

- **学习率**是一个调整参数，它决定了模型训练期间每次迭代（epoch）的步长。步长是您响应估计误差更新神经元权重的速度。使用反向传播误差方法更新模型权重时，梯度将从模型的输入节点通过神经元流向输出节点，然后确定误差并反向传播以更新神经元（模型）的权重。更新这些神经元权重的速度就是学习率。
- 如果学习率**很高**，那么模型权重更新得很快而且很频繁，模型会很快收敛，但它可能会超过真正的误差最小值。**这意味着一个更快收敛但错误的模型。**
- 如果学习**率低**，那么模型权重更新缓慢，模型将需要很长时间才能收敛，但不会超过真正的误差最小值。**这意味着一个更慢但更准确的模型。**

Q6：***如何判断你的模型是否存在梯度爆炸问题？***

- 以下现象表明你训练的模型可能存在梯度爆炸的问题，例如：模型无法很好地学习训练数据（模型损失一直很高）、模型训练不稳定（每一次更新，loss变化很大）、训练时loss变为NaN；
- 当发生以上现象时，可以深入挖掘模型看看是否有梯度爆炸的问题，以下现象可以确认发生梯度爆炸：训练阶段模型权重变为NaN；在训练期间，每个节点和层的误差梯度值始终高于`1.0`；

Q7：**知道哪些关于*Hyperparameters Tuning的*方法？**

- **随机搜索**：为超参数创建了一个可能值的网格。每次迭代都会尝试从该网格中随机组合超参数，记录性能，最后返回提供最佳性能的超参数组合
- **网格搜索**：类似于手动调优，为网格中指定的所有给定超参数值的每个排列建立模型，并评估和选择最佳模型。最后，它返回具有最佳超参数的最佳模型
- **贝叶斯优化：**贝叶斯优化使用先前对损失的观测，来确定下一个(最佳)的参数点来取样损失

引用：[https://www.geeksforgeeks.org/hyperparameter-tuning/](https://www.geeksforgeeks.org/hyperparameter-tuning/)

Q8：***在设计神经网络时，有哪些方法防止过拟合？***

- 简化模型：为了降低复杂性，我们可以简单地删除层或减少神经元数量以使网络更小。
- early-stopping：正则化的一种形式。NN使用梯度下降更新网络参数，但过了某个训练阶段，改进模型对训练数据的拟合会导致泛化误差增加。过了那个点，改进模型对训练数据的拟合会导致泛化误差增加。early-stopping规定了网络可以训练的迭代次数。

![Untitled](Model%20Evaluation%2033e6ecb893ba43b8aa236bcee2b86a1b/Untitled%202.png)

- 数据扩充：增加数据集中存在的图像数量。一些流行的图像增强技术包括翻转、平移、旋转、缩放、改变亮度、添加噪声
- 正则化：L1正则化以及L2正则化，L1正则化的目标是最小化权重的绝对值，L2正则化的目标是最小化权重的平方幅度
- 使用dropout策略；dropout在每次迭代的训练过程中随机从神经网络中删除神经元。当我们丢弃不同的神经元组时，相当于训练了不同的神经网络。不同的网络会以不同的方式过拟合，所以 dropout 的最终效果是减少过拟合

引用：[https://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html](https://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html)

Q9：***如何解决逻辑回归中的过拟合问题？***

- 减少使用的特征数量；丢弃那些提供信息量不大的特征
- 使用L2正则化

# 中级

Q10：***什么是混淆矩阵？***

Q11：***解释ROC曲线***

Q12：***解释AUC值***

Q13：***ROC曲线和AUC值如何帮助衡量模型的好坏？***

Q14：***使用AUC值衡量模型性能有什么好处和坏处？***

Q15：***解释什么是F1-Score***

Q16：***当训练数据存在类别间分布不平衡时，如何选择评估指标？***

Q17：***使用早停策略时，可能会有什么问题？***

# 高级

Q18：***算法A具有更高的Accuracy，算法B具有更高的Recall（召回率），如何判断算法A和算法B哪个更好？***

Q19：***什么是AIC指标？***

Q20：***什么是BIC指标？***