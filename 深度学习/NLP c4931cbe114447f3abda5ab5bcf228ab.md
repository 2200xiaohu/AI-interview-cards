# NLP

# 初级:

Q1：***NLP任务中，文本预处理有什么作用？***

- 文本预处理会将文本转换为机器能读懂的形式，以便机器学习算法可以更好地执行。并且，在情感分析等任务中，去除停用词等预处理有助于提高机器学习模型的准确率。
- 常见的文本预处理手段包括：删除 HTML 标签，删除停用词，删除数字，小写所有字母，词形还原等。

Q2：*****词形还原*和*词干*提取有什么区别？**

- **词干提取**只是删除*单词的最后几个字符*，通常会导致错误的含义和拼写，如`eating -> eat, Caring -> Car.`
- **词形还原**考虑*上下文*并将单词转换为其有意义的基本形式，如`Stripes -> Strip (verb) -or- Stripe (noun), better -> good`

Q3：***何时使用词形还原？何时使用词干提取？***

- 词干提取更多被应用于信息检索领域，如Solr、Lucene等，用于扩展检索，粒度较粗。词形还原更主要被应用于文本挖掘、自然语言处理，用于更细粒度、更为准确的文本分析和表达
- 词干提取计算量耗费小，适用于大数据集；词形还原耗费计算量大，常用于对准确性要求高的小数据集

Q4：***POS tagging（词性标注）有什么用？***

- **POS 标记**用于将每个单词分类到其词性中
- 词性可用于查找语法或词汇模式
- 在英语中，同一个词可以拥有不同的词性，词性标注有助于区分它们。

Q5：***使用词袋模型（Bag of words）提取特征有哪些优点？***

- 词袋模型非常简单和灵活，可以自己设计词汇表，可以以多种方式从文档中提取特征
- 由相似内容组成的文本在其他方面（例如含义）也将相似，因此词袋模型能反映句子含义间的相似性

Q6：***TF-IDF方法与TF方法的区别在哪里？***

- TF-IDF定义为词频-逆文档频率，TF定义为词频
- TF-IDF是一种统计方法，旨在反映一个词对语料库集合中文档的重要性：字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降；TF是一个词在文档中出现的次数的计数。
- TF定义为$tf_{ij}=\frac{n_{ij}}{\sum_{k}^{}n_{k,j}}$，$n_{ij}$是该词在文件中出现的次数，分母则是文件中所有词汇出现的次数总和；TF-IDF定义为$tfidf(t,d)=tf(t,d)*log(\frac{N}{df+1})$，其中$N$是语料库中的总文件数，$df$表示包含词语的文件数目

Q7：***什么是one-hot vector？它们如何用于自然语言处理？***

- one-hot 向量可用于以$L*N$大小矩阵的形式表示句子，其中$N$是语料库中单个单词的数量，$L$是句子的长度；将词语所在下标位置置为1，其他位置置为0

Q8：***文本预处理有哪些方法？***

- 文本预处理主要有三个不同类型：
- **标记化（Tokenization）**：这是将一组文本分成更小的部分或标记的过程。段落被标记化为句子，句子被标记化为单词。
- **规范化（Normalization）**：数据库规范化是将数据库的结构转换为一系列规范形式。它实现的是数据的组织，使其在所有记录和字段中看起来相似。同样，在NLP领域，规范化可以是将所有单词转换为小写的过程。这使得所有的句子和标记看起来都一样，并且不会使机器学习算法复杂化。
- **去噪（Noise Removal）**：是对文本进行清理的过程。做一些事情，比如删除不需要的字符，比如空格、数字、特殊字符等。

Q9：***one-hot vector应用在NLP领域有什么不足之处？***

- **矩阵稀疏和维度灾难**。one-hot表示是将词语所在下标位置置为1，其他位置置为0，而现实生活中，词语的集合是很大的，达到几千甚至几万，而每个向量的维度是和词语集合中词语的数量是一致的，所以一个词需要用几千甚至几万的维度来表示，如此大的维度在后续计算中需要很大的计算资源。此外，一个向量中只有一个维度是非零的，明显是过于稀疏的。
- **语义缺失**。在我们的表达中，词语之间是有一定的相似性的，例如“i”和“you”、“apple”和“banana”之间的相似性是比较高的，而“i”和“apple”之间的相似性比比较低的。而词向量作为词语的数字特征表示，理应需要保持词语之间语义上的相似性。但是，one-hot所得出来的每个词语的向量与其他词语的向量都是正交的，即每个词语之间的余弦相似度均为0，每对词语之间的欧式距离也是相同的。所以，这种向量表示失去了词语之间的相似性。

引用：https://www.jianshu.com/p/9948c5764302

Q10：***TF-IDF方法比TF方法好的地方有哪些？***

- TF方法只统计句子中词语出现的词频，词语出现的越多，重要性越高；而TF-IDF考虑到字词的重要性会随着它在语料库中出现的频率成反比下降，有利于分析出其中的关键词，降低一些出现频次高但重要性低词汇的重要性，如中文助词”的“。

Q11：***比较TF-IDF方法和Bag of words词袋模型***

- Bag of Words 只是创建一组向量，其中包含文档（评论）中单词出现的次数，而 TF-IDF 模型包含有关较重要单词和较不重要单词的信息。
- 词袋模型很容易解释，然而，TF-IDF 通常在机器学习模型中表现更好

Q12：***解释什么是BLEU值？***

- BLEU（*bilingual evaluation understudy*）是用于评估**模型生成的句子(candidate)**和**实际句子(reference)**的差异的指标，用于评估*机器翻译*文本的质量。BLEU实现是分别计算**candidate句**和**reference句**的**N-grams模型**, 然后统计其匹配的个数来计算得到

# 中级:

Q13：***NLP处理中，会碰到哪些歧义问题？***

Q14：***中文分词问题中，会碰到那些歧义问题类型？***

Q15：***在文本情感分析任务中，你会选用哪种loss？***

Q16：***解释什么是ROUGE指标？ROUGE与BLEU值有什么不同？***

Q17：***在机器翻译领域，为什么encoder-decoder结构的RNN取代了seq2seq RNN？***

Q18：***解释word embedding***

Q19：***word embedding的优点？***

Q20：***CNN在NLP中有哪些应用？CNN在NLP任务中应用的直觉是什么？***

Q21：***解释word2vec方法***

Q22：***word2vec与Glove算法有什么相同之处和区别？***

Q23：***解释隐马尔可夫模型（HMM）***

Q24：***bert的架构是什么 目标是什么，输入包括了什么 三个embedding输入是怎么综合的？***

Q25：***Seq2seq模型中decode和encode的差别有哪些？***

Q26：***阐述transformer的模型架构？***

Q26：***bert模型有哪些可以改进的地方？***

Q27：***讲述BPE模型***

Q28：***BPE模型与Wordpiece模型有什么区别？***

Q29：***BERT模型里，self-attention操作里$\sqrt{d_{k}}$的作用***

Q30：***attention机制为什么有效？***

Q31：***分析Bert模型存在的缺点***

Q32：***Transformer中残差结构的作用***

Q33：***Transformer采用postnorm还是prenorm？为什么？***

Q34：***BERT为什么用字粒度而不是用词粒度？***

Q35：***HMM 和 CRF 算法的原理和区别？***

Q36：***BiLSTM+CRF模型中，CRF层的作用？***

Q37：***nlp有哪些数据增强的方法？***

Q38：***NLP如何解决OOV问题？***

Q39：***分析Bert，不同层针对NLP的什么任务？***

Q40：***Albert里的SOP为什么会有效？***

# 高级:

Q41：***BERT的三个embedding为什么可以相加？***

Q42：**Word2vec和LDA两个模型有什么区别和联系？**

Q43：**相对位置编码和绝对位置编码有什么区别？**

Q44：**Elmo 的思想是什么？**

Q45：**word2vec中霍夫曼树是什么？**

Q46：**Word2vec 中 为什么要使用霍夫曼树？**

Q47：**word2vec和NNLM对比有什么区别？**

Q48：**word2vec和tf-idf 在相似度计算时的区别？**

Q49：**word2vec负采样有什么作用？**

Q50：**elmo、GPT、bert三者之间有什么区别？**

Created BY SUPRE IDOL