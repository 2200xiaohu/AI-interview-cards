# data processing

# 初级:

Q1：**特征工程与特征选择有什么区别？**

- 特征工程允许我们从已有的特征中创建新的特征，以帮助机器学习模型做出更有效和准确的预测。特征工程的任务是：
    - 填充变量中的缺失值。
    - 将分类变量编码为数字。
    - 变量转换。
    - 从数据集中可用的特征中创建或提取新特征。
- 特征选择允许我们从特征池中选择特征，有助于机器学习模型更有效地对目标变量进行预测。
- 典型的机器学习流水线中，我们在完成特征工程后进行特征选择

Q2：**协方差和相关性有什么区别？**

- 协方差衡量一个变量的变化是否导致另一个变量的变化，并处理数据集中仅变量的线性关系。它的值范围从负无穷到正无穷。简单的说，协方差表示变量之间线性关系的方向

![Untitled](data%20processing%20f3791b689b3c49b186899609e17e2c23/Untitled.png)

- 相关性衡量两个或多个变量彼此相关的强度，它的值介于-1到1之间。相关性也衡量两个变量之间线性关系的强度和方向，是协方差的函数。

Q3：**你会在大型数据集使用K-NN吗？为什么**

- 不建议在大型数据集上执行**K-NN**，因为计算和内存成本会增加；
- KNN的计算流程
    1. 首先计算训练集中所有向量的距离并存储它们。
    2. 对计算出的距离进行排序。
    3. 存储 K 个最近的向量。
    4. 计算出 K 个最近向量显示的最频繁的类。

Q4：**简述交叉验证方法**

- 交叉验证，就是重复的使用数据，把得到的样本数据进行切分，组合为不同的训练集和测试集，用训练集来训练模型，用测试集来评估模型预测的好坏
- 交叉验证用在数据不是很充足的时候

![Untitled](data%20processing%20f3791b689b3c49b186899609e17e2c23/Untitled%201.png)

Q5：**解释你是如何理解降维的**

- 降维是指降低数据的维度，使用更少更具辨别力的特征，可以描述数据中的大部分方差，从而保留大部分相关信息
- 有多种技术可用于执行此操作，包括但不限于PCA、ICA和Matrix Feature Factorization。

Q6：**解释Normalizing和scaling的区别**

- Normalizing是指归一化，会改变数据的分布情况，使转换后的数据大致呈正态分布
- scaling是指将数据乘以一个常数，不改变数据分布情况

Q7：**什么情况下，更少的训练数据会提高更高的模型准确性？**

- 从数据中删除冗余数据，例如：外观相似的图像，语义类似的句子

Q8：**如何处理不平衡的数据？**

- 不平衡的数据是指：其中一个或几个标签占数据集的大部分，而其他标签的示例则少得多
- 常用的平衡方法有：
    - 下采样：减少模型训练期间使用的多数类的示例数量。通常与 Ensemble 模式结合以获得更好的效果。
    - 上采样：通过复制少数类示例和生成额外的合成示例来增加少数类数量。
    
    ![Untitled](data%20processing%20f3791b689b3c49b186899609e17e2c23/Untitled%202.png)
    
    - 加权类别：通过加权类别，告诉模型在训练期间更重视少数标签类别

# 中级:

Q9：**特征插补值有哪些选择？**

Q10：**简述K折交叉验证的工作流程**

Q11：**为什么数据在高维空间更稀疏？**

Q12：***Bagging*和*Boosting*算法有什么区别？**

Q13：**使用决策树做模型有哪些缺点？如何解决？**

Q14：**在神经网络训练中，如何选择数据缩放scaling方法？**