# Optimisation

## 初级:

Q1：**解释什么是贝叶斯优化方法？**

- 贝叶斯优化是一种黑盒优化算法，用于求解表达式未知的函数的极值问题。算法根据一组采样点处的函数值预测出任意点处函数值的概率分布，这通过高斯过程回归而实现。根据高斯过程回归的结果构造采集函数，用于衡量每一个点值得探索的程度，求解采集函数的极值从而确定下一个采样点。最后返回这组采样点的极值作为函数的极值。
- 贝叶斯算法在机器学习中被用于AutoML算法，自动确定机器学习算法的超参数。某些NAS算法也使用了贝叶斯优化算法。

Q2：**解释机器学习中的凸优化**

- 对于机器学习来说，如果要优化的问题被证明是凸优化问题，则说明此问题可以被比较好的用梯度下降等方法解决
- 凸优化更快、更简单且计算量更少

![Untitled](Optimisation%200ac16f5c3b844d53a67b6c09c22455aa/Untitled.png)

引用：[https://stats.stackexchange.com/](https://stats.stackexchange.com/)

Q3：你知道哪些超参数调整（Hyperparameters tuning）方法?

- **随机搜索**：超参数创建了一个可能值的网格。每次迭代都会尝试从该网格中随机组合超参数，记录性能，最后返回提供最佳性能的超参数组合。
- **网格搜索：**将搜索空间定义为超参数值的网格，并评估网格中每个位置的算法性能（遍历），最后返回提供最佳性能的超参数组合。
- **贝叶斯优化**：假设超参数与最后我们需要优化的损失函数存在一个函数关系；通过SMBO等算法最小化函数，从而得到最优的参数组合

Q4：**解释Adam优化器**

- Adam：Adaptive Moment Estimation 利用梯度的一阶矩估计和二阶矩估计动态调节每个参数的学习率；
- Adam算法汇集了Momentum+ RMSProp算法

Q5：**Adam算法有什么局限性？**

- 虽然使用Adam进行训练有助于快速收敛，但结果模型的泛化性能往往不如使用SGD进行动量训练时的泛化性能。
- 另一个问题是，即使Adam有自适应学习率，当使用良好的学习率计划时，它的性能也会提高。特别是在训练的早期，使用较低的学习率来避免发散是有益的。这是因为在一开始，模型的权值是随机的，因此得到的梯度不是很可靠。如果学习率太大，可能会导致模型采取太大的步骤，而没有确定合适的权重。当模型克服了这些初始稳定性问题后，可以提高学习速度，加快收敛速度。

Q6：***Batch Size*如何影响梯度下降方法的收敛性？为什么？**

- 大Bacthsize训练倾向于收敛到接近初始参数值的最小值，而不是探索所有参数空间。
- 大Bacthsize训练倾向于收敛到更清晰的最小值，而小批量训练倾向于收敛到更平坦的最小值。

![Untitled](Optimisation%200ac16f5c3b844d53a67b6c09c22455aa/Untitled%201.png)

如图，两个最小值都达到相同的损失值，但平坦的最小值对参数空间中的扰动不太敏感。他们提供的实验证据表明，大批量训练更有可能收敛到尖锐的最小值和接近起点的最小值。小批量训练中的固有噪声有助于将参数推出尖锐的盆地。

引用：

Q7：**什么情况下，选择遗传算法作为优化算法？**

- 当许多处理器可以并行使用时
- 当目标函数具有高模态（许多局部最优）时

引用：[https://stats.stackexchange.com/questions/249471/when-are-genetic-algorithms-a-good-choice-for-optimization](https://stats.stackexchange.com/questions/249471/when-are-genetic-algorithms-a-good-choice-for-optimization)

Q8：**Adagrad算法是怎样调整学习率的？**

- AdaGrad是一个基于梯度的优化算法，它的主要特点是：它对不同的参数调整学习率，具体而言，对低频出现的参数进行大的更新，对高频出现的参数进行小的更新。 因此，他很适合于处理稀疏数据。具体计算过程如下：
- 计算梯度:

$$
g_t = \nabla_\theta J(\theta_{t-1})
$$

- 累计平方梯度：

$$
r_t = r_{t-1} + g_t \odot g_t
$$

- 计算梯度更新：

$$
\Delta \theta = {\eta \over \epsilon + \sqrt{r_t}} \odot g_t
$$

- 更新学习率：

$$
\theta_t=\theta_{t-1} - \Delta \theta
$$

# 中级:

Q9：**比较牛顿法和梯度下降**

Q10：**贝叶斯优化如何在超参数优化中使用？**

Q11：**剪枝方法是否适用于神经网络优化？**

Q12：**在随机森林模型中，如何优化森林中树的数量？**

Q13：****若CNN网络很庞大，在手机上运行效率不高，对应模型压缩方法有了解吗？****

Q14：**讲述模型蒸馏的原理与操作**
