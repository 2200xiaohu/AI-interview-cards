# Q-learning

Q1：**怎样定义强化学习中的状态？**

- 强化学习（RL）中的状态表示问题类似于有监督或无监督学习中的特征表示、特征选择和特征工程问题。对复杂问题建模的常用方法是离散化，但离散化有可能会带来维度灾难问题。对于这些问题，通常会利用状态表示学习（SRL）,将状态表示为不同特征的向量

Q2：**Q Learning 中的*Alpha*和*Gamma*参数代表什么？**

- Alpha是学习率，随着继续获得越来越大的知识库，它应该会降低。学习率应该在 的范围内`0-1`。学习率越高，它会很快替换新的 Q 值，因此我们需要以某种方式对其进行优化，以便我们的智能体从以前的 Q 值中学习。学习率是一种工具，可用于确定我们保留了多少我们需要为状态-动作对保留的先前经验知识。
- Gamma是未来奖励的价值。它会对学习产生很大影响，可以是动态值或静态值。如果等于`1`，则代理人对未来奖励的重视程度与对当前奖励的重视程度相同。这意味着，在十个动作中，如果代理人做了好事，这与直接执行此动作一样有价值。所以学习在高gamma值下效果不佳。相反，gamma将`0`导致代理只重视即时奖励，这只适用于非常详细的奖励功能。

Q3：**怎么判断 Q-Learning 算法何时收敛？**

- 当学习曲线变得平坦并且不再增加时，强化学习算法被认为收敛。但是由于探索参数ε不是逐渐增加的，在达到最优策略之前，Q-Learning 会以过早的方式收敛。
- 因此，必须满足两个条件才能保证极限收敛：
    - **学习率接近于零，但不能以太快的速度**。形式上，这要求学习率之和必须发散，但它们的平方和必须收敛。具有这些属性的示例序列是1/1, 1/2, 1/3, 1/4, ....
    - **每个状态-动作对必须被无限频繁地访问**。这有一个精确的数学定义：每个动作在每个状态下都必须有一个非零的概率被策略选择，即π(s, a) > 0对于所有(s, a)。在实践中，使用ε-贪心策略(where ε > 0) 可确保满足此条件。

引用：[https://stats.stackexchange.com/](https://stats.stackexchange.com/)

Q4：**如何理解epsilon greedy 算法？**

- (1) 我们的小车一开始接触到的 state 很少，并且如果小车按照已经学到的 qtable 执行，那么小车很有可能出错或者绕圈圈。同时我们希望小车一开始能随机的走一走，接触到更多的 state。
- (2) 基于上述原因，我们希望小车在一开始的时候不完全按照 Q learning 的结果运行，即以一定的概率 epsilon，随机选择 action，而不是根据 maxQ 来选择 action。然后随着不断的学习，那么我会降低这个随机的概率，使用一个衰减函数来降低 epsilon。
- (3) 这个就解决了所谓的 exploration and exploitation 的问题，在“探索”和“执行”之间寻找一个权衡。

引用：[https://blog.csdn.net/Gin07](https://blog.csdn.net/Gin07)

Q5：**解释Q-learning算法**

- Q(s, a)函数，即质量函数，用来表示智能体在s状态下采用a动作并在之后采取最优动作条件下
的打折的未来奖励(先不管未来的动作如何选择)：

$$
Q(s_t,a_t)=max(R_{t+1})
$$

假设有了这个Q函数，那么我们就能够求得在当前 t 时刻当中，做出各个决策的最大收益值，通过对比这些收益值，就能够得到 t 时刻某个决策是这些决策当中收益最高。根据Q函数的递推公式可以得到：

$$
Q(s_t,a_t)=r+max_aQ(s_{t+1},a_{t+1})
$$

这就是注明的贝尔曼公式。贝尔曼公式是说，对于某个状态来讲，最大化未来奖励相当于 最大化即刻奖励与下一状态最大未来奖励之和。Q-learning的核心思想是：我们能够通过贝尔曼公式迭代地近似Q-函数。

Q6：**分别写出基于状态值函数的贝尔曼方程以及基于状态-动作值函数的贝尔曼方程。**

- 基于状态值函数的贝尔曼方程：

$$
{{V}^{\pi }}(s)={{\mathbb{E}}_{a \sim \pi (a|s)}}{{\mathbb{E}}_{s'\sim p(s'|s,a)}}[r(s,a,s')+\gamma {{V}^{\pi }}(s')]
$$

- 基于状态-动作值函数(Q函数)的贝尔曼方程：

$$
{{Q}^{\pi }}(s,a)={{\mathbb{E}}_{s'\sim p(s'|s,a)}}\left[ r(s,a,s')+\gamma {{\mathbb{E}}_{a'\sim \pi (a'|s')}}[{{Q}^{\pi }}(s',a')] \right]
$$

Q7：**讲一下SARSA，最好可以写出其Q(s,a)的更新公式。另外，它是on-policy还是off-policy，为什么？**

- SARSA可以算是Q-learning的改进，其更新公式为：

$$
Q(s, a) \larr Q(s, a) + \alpha [r(s,a) + \gamma Q(s', a') - Q(s, a)]
$$

其为on-policy的，SARSA必须执行两次动作得到(s,a,r,s′,a′)才可以更新一次；而且a′是在特定策略π
的指导下执行的动作，因此估计出来的Q(s,a)是在该策略 π之下的Q值，样本生成用的π和估计的π
是同一个，因此是on-policy。

Q8：**请问value-based和policy-based方法的区别是什么？**

1. 生成策略上的差异，前者确定，后者随机。基于价值的方法中动作-价值对的估计值最终会收敛（通常是不同的数，可以转化为0～1 的概率），因此通常会获得一个确定的策略；基于策略的方法不会收敛到一个确定的值，另外他们会趋向于生成最佳随机策略。如果最佳策略是确定的，那么最优动作对应的值函数的值将远大于次优动作对应的值函数的值，值函数的大小代表概率的大小。
2. 动作空间是否连续，前者离散，后者连续。基于价值的方法，对于连续动作空间问题，虽然可以将动作空间离散化处理，但离散间距的选取不易确定。过大的离散间距会导致算法取不到最优动作，会在最优动作附近徘徊；过小的离散间距会使得动作的维度增大，会和高维度动作空间一样导致维度灾难，影响算法的速度。而基于策略的方法适用于连续的动作空间，在连续的动作空间中，可以不用计算每个动作的概率，而是通过正态分布选择动作。
3. 基于价值的方法，例如Q学习算法，是通过求解最优价值函数而间接地求解最优策略；基于策略的方法，例如REINFORCE等算法直接将策略参数化，通过策略搜索、策略梯度或者进化方法来更新参数以最大化回报。基于价值的方法不易扩展到连续动作空间，并且当同时采用非线性近似、自举等策略时会有收敛问题。策略梯度具有良好的收敛性。
4. 另外，对于价值迭代和策略迭代，策略迭代有两个循环，一个是在策略估计的时候，为了求当前策略的价值函数需要迭代很多次；另一个是外面的大循环，即策略评估、策略提升。价值迭代算法则是一步到位，直接估计最优价值函数，因此没有策略提升环节。

引用：《Easy RL》

Q9：**能否简单说下动态规划、蒙特卡洛和时序差分的异同点？**

- 相同点：都用于进行价值函数的描述与更新，并且所有方法都基于对未来事件的展望计算一个回溯值。
- 不同点：蒙特卡洛方法和时序差分方法属于model-free方法，而动态规划属于model-based方法；时序差分方法和蒙特卡洛方法，因为都是model-free的方法，所以对于后续状态的获知也都是基于试验的方法；时序差分方法和动态规划方法的策略评估，都能基于当前状态的下一步预测情况来得到对于当前状态的价值函数的更新。
- 另外，时序差分方法不需要等到试验结束后才能进行当前状态的价值函数的计算与更新，而蒙特卡洛方法需要与环境交互，产生一整条马尔可夫链并直到最终状态才能进行更新。时序差分方法和动态规划方法的策略评估不同之处为model-free和model-based，动态规划方法可以凭借已知转移概率推断出后续的状态情况，而时序差分方法借助试验才能知道。
- 蒙特卡洛方法和时序差分方法的不同在于，蒙特卡洛方法进行了完整的采样来获取长期的回报值，因而在价值估计上会有更小的偏差，但是也正因为收集了完整的信息，所以价值的方差会更大，原因在于其基于试验的采样得到，和真实的分布有差距，不充足的交互导致较大方差。而时序差分方法则相反，因为它只考虑了前一步的回报值，其他都是基于之前的估计值，因而其价值估计相对来说具有偏差大方差小的特点。

Q10：**简述DQN？**

- 深度Q网络(DQN)是基于深度学习的Q学习算法，其结合了价值函数近似(Value Function Approximation)与神经网络技术，并采用了目标网络(Target Network)和经验回放(Experience Replay)技巧进行网络的训练。

Q11：**DQN中的两个技巧：目标网络(target network)和经验回放(experience replay)的具体作用是什么呢？**

- 在DQN中某个动作值函数的更新依赖于其他动作值函数。如果我们一直更新值网络的参数，会导致更新目标不断变化，也就是我们在追逐一个不断变化的目标，这样势必会不太稳定。 为了解决在基于时序差分的网络的问题时，优化目标$\mathrm{Q}^{\pi}\left(s_{t}, a_{t}\right) =r_{t}+\mathrm{Q}^{\pi}\left(s_{t+1}, \pi\left(s_{t+1}\right)\right)$左右两侧会同时变化使得训练过程不稳定，从而增大回归的难度。目标网络选择将上式的右部分即$r_{t}+\mathrm{Q}^{\pi}\left(s_{t+1}, \pi\left(s_{t+1}\right)\right)$固定，通过改变上式左部分的网络的参数，进行回归。
- 对于经验回放，其会构建一个回放缓冲区(Replay Buffer, Replay Memory)，用来保存许多数据，每一个数据的形式如下：在某一个状态$s_t$，采取某一个动作$a_t$，得到了奖励$r_t$，然后跳到状态$a_{t+1}$。我们使用$\pi$去跟环境互动很多次，把收集到的数据都放到这个回放缓冲区中。当我们的回放缓冲区”装满“后，就会自动删去最早进入缓冲区的数据。在训练时，对于每一轮迭代都有相对应的批量（与我们训练普通网络一样，通过采样得到），然后用这个批量中的数据去更新Q函数。即Q函数在采样和训练的时候会用到过去的经验数据，也可以消除样本之间的相关性。

引用：[https://www.cnblogs.com/kailugaji/p/15354491.html#_label3_0_1_3](https://www.cnblogs.com/kailugaji/p/15354491.html#_label3_0_1_3)

Q12：**DQN（Deep Q-learning）和Q-learning有什么异同点？**

- 整体来说，从名称就可以看出，两者的目标价值以及价值的更新方式基本相同，另外一方面，不同点在于：
1. 首先，DQN 将 Q-learning 与深度学习结合，用深度网络来近似动作价值函数，而 Q-learning 则是采用表格存储。
2. DQN 采用了我们前面所描述的经验回放（Experience Replay）训练方法，从历史数据中随机采样，而 Q-learning 直接采用下一个状态的数据进行学习。

Q13：**DQN都有哪些变种？**

- 深度Q网络(DQN)有3 个经典的变种：双深度Q网络(Double DQN)、竞争深度Q网络(Dueling DQN)、优先级双深度Q网络(Prioritized Replay Buffer)。

1）双深度Q网络：将动作选择和价值估计分开，避免Q值被过高估计。

2）竞争深度Q网络：将Q值分解为状态价值和优势函数，得到更多有用信息。

3）优先级双深度Q网络：将经验池中的经验按照优先级进行采样

Q14：**简述Double DQN(双深度Q网络)原理？**

- 深度Q网络(DQN)由于总是选择当前最优的动作价值函数来更新当前的动作价值函数，因此存在过估计问题（估计的价值函数值大于真实的价值函数值）。为了解耦这两个过程，双深度Q网络使用两个价值网络，一个网络用来执行动作选择，然后用另一个网络的价值函数对应的动作值更新当前网络。

Q15：**请问Dueling DQN(竞争深度Q网络)模型有什么优势？**

- 对于我们的 Q(s,a)，其对应的状态由于为表格的形式，所以是离散的，而实际中的状态大都不是离散的。对于 Q(s,a)的计算公式， Q(s,a)=V(s)+A(s,a)；其中的 V(s)是对于不同的状态都有值，A(s,a)对于不同的状态都有不同的动作对应的值。所以本质上来说，我们最终的矩阵 Q(s,a)的结果是将每一个 V(s)加到矩阵 A(s,a)中得到的。从模型的角度考虑，我们的网络直接改变的 Q(s,a)而是更改的 V、A。但是有时我们更新时不一定会将 V(s)和 Q(s,a)都更新。我们将其分成两个部分后，就不需要将所有的状态-动作对都采样一遍，我们可以使用更高效的估计Q值的方法将最终的 Q(s,a)计算出来。

Q16：**请问Actor - Critic有何优点？**

1. 相比以值函数为中心的算法，Actor - Critic应用了策略梯度的做法，这能让它在连续动作或者高维动作空间中选取合适的动作，而 Q-learning 做这件事会很困难甚至瘫痪。
2. 相比单纯策略梯度，Actor - Critic应用了Q-learning或其他策略评估的做法，使得Actor - Critic能进行单步更新而不是回合更新，比单纯的策略梯度的效率要高。

Q17：**简述一下DDPG算法？**

- 深度确定性策略梯度算法(Deep Deterministic Policy Gradient，简称 DDPG)使用演员-评论员(Actor - Critic)结构，但是输出的不是动作的概率，而是具体动作，其可以用于连续动作的预测。优化的目的是将深度Q 网络(DQN)扩展到连续的动作空间。另外，其含义如其名：

1）深度(Deep)是因为用了深度神经网络；

2）确定性(Deterministic)表示其输出的是一个确定的动作，可以用于连续动作的环境；

3）策略梯度(Policy Gradient)代表的是它用到的是策略网络。REINFORCE算法每个回合(episode)就会更新一次网络，但是深度确定性策略梯度算法每个步骤(step)都会更新一次策略网络，它是一个单步更新的策略网络。

Q18：**请问DDPG是on-policy还是off-policy，为什么？**

- 异策略(off-policy)算法。因为：

1）深度确定性策略梯度算法(DDPG)是优化的深度Q网络(DQN)，其使用了经验回放，所以为异策略算法。

2）因为深度确定性策略梯度算法(DDPG)为了保证一定的探索，对输出动作加了一定的噪声，行为策略不再是优化的策略。

引用：[https://www.cnblogs.com/kailugaji](https://www.cnblogs.com/kailugaji)